{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Simple RNN from Scratch in Python (No Libraries)\n",
        "This notebook demonstrates how a basic **Recurrent Neural Network (RNN)** works for word prediction using only **core Python**. We use simple sentences as training data and implement each part manually â€” from forward pass to prediction.\n",
        "\n",
        "The goal is to predict the **next word** in a sentence based on the current one.\n",
        "\n",
        "---\n",
        "**Key Concepts:**\n",
        "- One-hot encoding for words\n",
        "- Hidden state memory (recurrent structure)\n",
        "- `tanh` activation and `softmax` output\n",
        "- Matrix operations manually coded\n"
      ],
      "metadata": {
        "id": "hkWe1alrSHnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zn5G8PN9sUtu"
      },
      "outputs": [],
      "source": [
        "# Sample training data: simple sentences\n",
        "data = [\n",
        "    \"hello how are you\",\n",
        "    \"how are you doing\",\n",
        "    \"are you doing well\"\n",
        "]\n",
        "\n",
        "# Build vocabulary and mapping\n",
        "words = set(\" \".join(data).split())\n",
        "word_to_idx = {word: i for i, word in enumerate(words)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§¾ Why Word Indexing?\n",
        "We need to convert text into numerical format so the RNN can process it.\n",
        "\n",
        "- We create a **word-to-index mapping** to assign a unique integer to each word in our vocabulary.\n",
        "- Later, we use **one-hot encoding** to convert each word into a vector.\n",
        "\n",
        "ðŸ§® Vocabulary size = total unique words in dataset.\n"
      ],
      "metadata": {
        "id": "Og6bbOAMSS10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "# Initialize RNN parameters (weights and biases)\n",
        "def init_weights(hidden_size, vocab_size):\n",
        "    Wx = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(vocab_size)]  # Input to hidden\n",
        "    Wh = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(hidden_size)] # Hidden to hidden\n",
        "    Wy = [[random.uniform(-1, 1) for _ in range(vocab_size)] for _ in range(hidden_size)]  # Hidden to output\n",
        "    bh = [0] * hidden_size  # Hidden bias\n",
        "    by = [0] * vocab_size   # Output bias\n",
        "    return Wx, Wh, Wy, bh, by\n"
      ],
      "metadata": {
        "id": "NyfWdzFWSWGi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ï¸ Why These Weights and Biases?\n",
        "\n",
        "We need trainable parameters for the RNN:\n",
        "\n",
        "- **Wx**: Input-to-hidden weights (shapes the input word into hidden form)\n",
        "- **Wh**: Hidden-to-hidden weights (keeps memory of past inputs)\n",
        "- **Wy**: Hidden-to-output weights (maps hidden state to output prediction)\n",
        "- **bh, by**: Bias vectors for hidden and output layers\n",
        "\n",
        "ðŸ”¢ These are initialized randomly, as in real neural networks.\n"
      ],
      "metadata": {
        "id": "1d747CTGSZfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation function: tanh\n",
        "def tanh(x):\n",
        "    return [math.tanh(i) for i in x]\n",
        "\n",
        "# Softmax for output prediction\n",
        "def softmax(x):\n",
        "    e_x = [math.exp(i) for i in x]\n",
        "    total = sum(e_x)\n",
        "    return [i / total for i in e_x]\n",
        "\n",
        "# Matrix-vector multiplication\n",
        "def matvec_mul(mat, vec):\n",
        "    return [sum(m * v for m, v in zip(row, vec)) for row in mat]\n",
        "\n",
        "# Vector addition\n",
        "def vec_add(v1, v2):\n",
        "    return [a + b for a, b in zip(v1, v2)]\n"
      ],
      "metadata": {
        "id": "TYdS0_W5SWDT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§® Why `tanh` and `softmax`?\n",
        "\n",
        "- **`tanh` activation** is used to control the hidden state: it outputs values between -1 and 1, which helps smooth gradients during learning.\n",
        "  \n",
        "  $$\n",
        "  \\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "  $$\n",
        "\n",
        "- **`softmax`** converts raw output scores into probabilities:\n",
        "\n",
        "  $$\n",
        "  \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
        "  $$\n",
        "\n",
        "ðŸ”¢ This lets us pick the most likely next word.\n"
      ],
      "metadata": {
        "id": "BhgYUQ_CSdXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One RNN time step\n",
        "def rnn_step(x, h_prev, Wx, Wh, Wy, bh, by):\n",
        "    h_t = tanh(vec_add(matvec_mul(Wx, x), vec_add(matvec_mul(Wh, h_prev), bh)))\n",
        "    y_t = vec_add(matvec_mul(Wy, h_t), by)\n",
        "    y_pred = softmax(y_t)\n",
        "    return h_t, y_pred\n"
      ],
      "metadata": {
        "id": "G5kznKxTSWAR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ” What happens in one RNN step?\n",
        "\n",
        "Given input word vector `x` and previous hidden state `h_prev`:\n",
        "\n",
        "1. **Hidden state update**:\n",
        "   \\[\n",
        "   h_t = \\tanh(W_x \\cdot x + W_h \\cdot h_{\\text{prev}} + b_h)\n",
        "   \\]\n",
        "\n",
        "2. **Output calculation**:\n",
        "   \\[\n",
        "   y = W_y \\cdot h_t + b_y\n",
        "   \\]\n",
        "\n",
        "3. **Prediction (probability)**:\n",
        "   \\[\n",
        "   \\text{softmax}(y)\n",
        "   \\]\n",
        "\n",
        "This is how memory flows across words!\n",
        "### ðŸ” What happens in one RNN step?\n",
        "\n",
        "Given input word vector `x` and previous hidden state `h_prev`:\n",
        "\n",
        "1. **Hidden state update**:\n",
        "   $$\n",
        "   h_t = \\tanh(W_x \\cdot x + W_h \\cdot h_{\\text{prev}} + b_h)\n",
        "   $$\n",
        "\n",
        "2. **Output calculation**:\n",
        "  $$\n",
        "   y = W_y \\cdot h_t + b_y\n",
        "  $$\n",
        "\n",
        "3. **Prediction (probability)**:\n",
        "  $$\n",
        "   \\text{softmax}(y)\n",
        " $$\n",
        "\n",
        "This is how memory flows across words!\n"
      ],
      "metadata": {
        "id": "loWg1hkASlZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode word\n",
        "def one_hot(index, size):\n",
        "    vec = [0] * size\n",
        "    vec[index] = 1\n",
        "    return vec\n",
        "\n",
        "hidden_size = 8\n",
        "Wx, Wh, Wy, bh, by = init_weights(hidden_size, vocab_size)\n",
        "\n",
        "# Simple training loop (forward only)\n",
        "for sentence in data:\n",
        "    words_list = sentence.split()\n",
        "    h_prev = [0] * hidden_size\n",
        "\n",
        "    for i in range(len(words_list) - 1):\n",
        "        current_word = words_list[i]\n",
        "        next_word = words_list[i + 1]\n",
        "\n",
        "        x = one_hot(word_to_idx[current_word], vocab_size)\n",
        "        h_prev, y_pred = rnn_step(x, h_prev, Wx, Wh, Wy, bh, by)\n",
        "\n",
        "        predicted_idx = y_pred.index(max(y_pred))\n",
        "        predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "        print(f\"Input: '{current_word}' â†’ Predicted: '{predicted_word}' (Target: '{next_word}')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7fO5gB_SV9L",
        "outputId": "76b6ff75-ee38-4202-ccea-4576681e59b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'hello' â†’ Predicted: 'doing' (Target: 'how')\n",
            "Input: 'how' â†’ Predicted: 'are' (Target: 'are')\n",
            "Input: 'are' â†’ Predicted: 'how' (Target: 'you')\n",
            "Input: 'how' â†’ Predicted: 'how' (Target: 'are')\n",
            "Input: 'are' â†’ Predicted: 'hello' (Target: 'you')\n",
            "Input: 'you' â†’ Predicted: 'hello' (Target: 'doing')\n",
            "Input: 'are' â†’ Predicted: 'doing' (Target: 'you')\n",
            "Input: 'you' â†’ Predicted: 'hello' (Target: 'doing')\n",
            "Input: 'doing' â†’ Predicted: 'hello' (Target: 'well')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸŽ¯ What Does the Output Mean?\n",
        "\n",
        "Each line shows:\n",
        "\n",
        "- The **current input word**\n",
        "- The **predicted next word** from the RNN\n",
        "- The **actual target** next word from sentence\n",
        "\n",
        "This shows how well the RNN has learned patterns, even **without backpropagation** (we're just forwarding and predicting here).\n"
      ],
      "metadata": {
        "id": "nl-ExAiCSwOE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0OBpM3leH0Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgGWX8ZaJsFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸ” Backpropagation in RNN â€“ Explained with Math and Chain Rule\n",
        "\n",
        "In this section, we walk through the **backward pass (backpropagation)** in a simple RNN â€” to update weights using the error between predicted and target output.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ Goal of Backpropagation\n",
        "To compute gradients (âˆ‚Loss/âˆ‚W) for all weights so we can **reduce the loss** using **gradient descent**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”„ Forward Recap\n",
        "At each time step:\n",
        "\n",
        "1. Hidden state update:\n",
        "$$\n",
        "h_t = \\tanh(W_x \\cdot x + W_h \\cdot h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "2. Output prediction:\n",
        "$$\n",
        "y = W_y \\cdot h_t + b_y\n",
        "$$\n",
        "\n",
        "3. Softmax for probabilities:\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(y)\n",
        "$$\n",
        "\n",
        "4. Loss (Cross-Entropy):\n",
        "$$\n",
        "L = -\\log(\\hat{y}_{\\text{target}})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”™ Backpropagation: Step by Step\n",
        "\n",
        "We use **chain rule** to propagate gradients backward through each operation.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 1. Gradient of Loss w.r.t Softmax Output (Logits)\n",
        "\n",
        "Let:\n",
        "- \\( \\hat{y} \\) be softmax output\n",
        "- \\( y_{\\text{target}} \\) be the true one-hot label\n",
        "\n",
        "The derivative of cross-entropy loss:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\hat{y}_i - y_{\\text{target},i}\n",
        "$$\n",
        "\n",
        "This is done via:\n",
        "```python\n",
        "dy = y_pred[:]      # Clone prediction\n",
        "dy[target_idx] -= 1 # Subtract 1 at true class\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 2. Gradient w\\.r.t. Output Layer Weights (Wy)\n",
        "\n",
        "We compute:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_y} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W_y}\n",
        "\\Rightarrow \\text{outer product of } dy \\text{ and } h_t\n",
        "$$\n",
        "\n",
        "Each element:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_y[i][j]} = dy[i] \\cdot h_t[j]\n",
        "$$\n",
        "\n",
        "```python\n",
        "for i_v in range(vocab_size):\n",
        "    for j_h in range(hidden_size):\n",
        "        Wy[i_v][j_h] -= learning_rate * dy[i_v] * h_t[j_h]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 3. Gradient w\\.r.t. Output Bias (by)\n",
        "\n",
        "Since:\n",
        "\n",
        "$$\n",
        "y = W_y \\cdot h_t + b_y \\Rightarrow \\frac{\\partial L}{\\partial b_y[i]} = dy[i]\n",
        "$$\n",
        "\n",
        "```python\n",
        "for i_v in range(vocab_size):\n",
        "    by[i_v] -= learning_rate * dy[i_v]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 4. Gradient w\\.r.t. Hidden State $h_t$\n",
        "\n",
        "We propagate error from output back to hidden state:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial h_t[j]} = \\sum_{i} W_y[i][j] \\cdot dy[i]\n",
        "$$\n",
        "\n",
        "```python\n",
        "dh = [sum(Wy[i_v][j_h] * dy[i_v] for i_v in range(vocab_size)) for j_h in range(hidden_size)]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 5. Derivative of tanh Activation\n",
        "\n",
        "Since:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(z), \\quad \\frac{d}{dz} \\tanh(z) = 1 - \\tanh^2(z)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial h_t} \\cdot (1 - h_t^2)\n",
        "$$\n",
        "\n",
        "```python\n",
        "dh_raw = [dh[j_h] * (1 - h_t[j_h] ** 2) for j_h in range(hidden_size)]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 6. Gradient w\\.r.t. Input Weights (Wx)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_x[i][j]} = dh_{\\text{raw}}[i] \\cdot x[j]\n",
        "$$\n",
        "\n",
        "```python\n",
        "for i_h in range(hidden_size):\n",
        "    for j_v in range(vocab_size):\n",
        "        Wx[i_h][j_v] -= learning_rate * dh_raw[i_h] * x[j_v]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 7. Gradient w\\.r.t. Hidden Bias (bh)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b_h[i]} = dh_{\\text{raw}}[i]\n",
        "$$\n",
        "\n",
        "```python\n",
        "for i_h in range(hidden_size):\n",
        "    bh[i_h] -= learning_rate * dh_raw[i_h]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 8. Gradient w\\.r.t. Recurrent Weights (Wh)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_h[i][j]} = dh_{\\text{raw}}[i] \\cdot h_{\\text{prev}}[j]\n",
        "$$\n",
        "\n",
        "```python\n",
        "for i_h in range(hidden_size):\n",
        "    for j_h_prev in range(hidden_size):\n",
        "        Wh[i_h][j_h_prev] -= learning_rate * dh_raw[i_h] * h_prev[j_h_prev]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‰ Why This Matters\n",
        "\n",
        "Backpropagation updates all weights to minimize the loss between predicted and target word. Without this step, the model wonâ€™t **learn or improve**.\n",
        "\n",
        "Every gradient step uses the **chain rule** to flow error backward:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "âœ… With each epoch, we reduce the total loss and improve predictions!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcMCMvJ_UUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# ========== 1. Prepare data ==========\n",
        "data = [\n",
        "    \"hello how are you\",\n",
        "    \"how are you doing\",\n",
        "    \"are you doing well\"\n",
        "]\n",
        "\n",
        "# Word to index mapping\n",
        "words = sorted(set(\" \".join(data).split()))\n",
        "word_to_idx = {w: i for i, w in enumerate(words)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "vocab_size = len(words)\n",
        "print(f\"Vocabulary Size: {vocab_size}\") # Debug print\n",
        "print(f\"Words: {words}\") # Debug print\n",
        "\n",
        "# One-hot encoding\n",
        "def one_hot(index, size):\n",
        "    vec = [0] * size\n",
        "    vec[index] = 1\n",
        "    return vec\n",
        "\n",
        "# ========== 2. Helper functions ==========\n",
        "def matvec_mul(matrix, vector):\n",
        "    # Multiply matrix (MxN) by vector (N,) -> result (M,)\n",
        "    # result[i] = sum_j matrix[i][j] * vector[j]\n",
        "    return [sum(m * v for m, v in zip(row, vector)) for row in matrix]\n",
        "\n",
        "def vec_add(v1, v2):\n",
        "    return [a + b for a, b in zip(v1, v2)]\n",
        "\n",
        "def tanh(vec):\n",
        "    return [math.tanh(v) for v in vec]\n",
        "\n",
        "# Note: dtanh calculation integrated below for efficiency\n",
        "\n",
        "def softmax(x):\n",
        "    # Numerical stability fix\n",
        "    max_x = max(x)\n",
        "    e_x = [math.exp(i - max_x) for i in x] # Subtract max for stability\n",
        "    total = sum(e_x)\n",
        "    if total == 0:\n",
        "        return [1.0 / len(x)] * len(x) # Avoid division by zero\n",
        "    return [i / total for i in e_x]\n",
        "\n",
        "def cross_entropy(predicted, target_idx):\n",
        "    # Add small epsilon to prevent log(0)\n",
        "    return -math.log(predicted[target_idx] + 1e-9)\n",
        "\n",
        "# ========== 3. Initialize parameters ==========\n",
        "hidden_size = 8\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Weight matrix dimensions: [Output_Size][Input_Size] for matvec_mul(W, v)\n",
        "# Wx: Input to Hidden (hidden_size x vocab_size)\n",
        "Wx = [[random.uniform(-0.1, 0.1) for _ in range(vocab_size)] for _ in range(hidden_size)]\n",
        "# Wh: Hidden to Hidden (hidden_size x hidden_size)\n",
        "Wh = [[random.uniform(-0.1, 0.1) for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
        "# Wy: Hidden to Output (vocab_size x hidden_size)\n",
        "Wy = [[random.uniform(-0.1, 0.1) for _ in range(hidden_size)] for _ in range(vocab_size)]\n",
        "\n",
        "bh = [0] * hidden_size\n",
        "by = [0] * vocab_size\n",
        "\n",
        "# Debug prints for dimensions\n",
        "print(f\"Wx dimensions: {len(Wx)} x {len(Wx[0]) if Wx else 0} (expected {hidden_size} x {vocab_size})\")\n",
        "print(f\"Wh dimensions: {len(Wh)} x {len(Wh[0]) if Wh else 0} (expected {hidden_size} x {hidden_size})\")\n",
        "print(f\"Wy dimensions: {len(Wy)} x {len(Wy[0]) if Wy else 0} (expected {vocab_size} x {hidden_size})\")\n",
        "print(f\"bh dimensions: {len(bh)} (expected {hidden_size})\")\n",
        "print(f\"by dimensions: {len(by)} (expected {vocab_size})\")\n",
        "\n",
        "\n",
        "# ========== 4. Training ==========\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for sentence in data:\n",
        "        words_list = sentence.split()\n",
        "        h_prev = [0] * hidden_size\n",
        "\n",
        "        for i in range(len(words_list) - 1):\n",
        "            x_word = words_list[i]\n",
        "            target_word = words_list[i + 1]\n",
        "\n",
        "            x = one_hot(word_to_idx[x_word], vocab_size)\n",
        "            target_idx = word_to_idx[target_word]\n",
        "\n",
        "            # ------ Forward ------\n",
        "            h_in = vec_add(matvec_mul(Wx, x), vec_add(matvec_mul(Wh, h_prev), bh))\n",
        "            h_t = tanh(h_in)\n",
        "            y_raw = vec_add(matvec_mul(Wy, h_t), by)\n",
        "            y_pred = softmax(y_raw)\n",
        "\n",
        "            loss = cross_entropy(y_pred, target_idx)\n",
        "            total_loss += loss\n",
        "\n",
        "            # ------ Backward ------\n",
        "            dy = y_pred[:]\n",
        "            # Check size consistency\n",
        "            if len(dy) != vocab_size:\n",
        "                raise ValueError(f\"Softmax output size mismatch at epoch {epoch}! Expected {vocab_size}, got {len(dy)}\")\n",
        "\n",
        "            dy[target_idx] -= 1  # gradient of cross-entropy w.r.t. logits (y_raw)\n",
        "\n",
        "            # Gradients w.r.t. Wy and by\n",
        "            # dWy = outer(dy, h_t) -> dWy[i][j] = dy[i] * h_t[j]\n",
        "            # Wy shape: [vocab_size][hidden_size]\n",
        "            # dy shape: [vocab_size]\n",
        "            # h_t shape: [hidden_size]\n",
        "            for i_v in range(vocab_size): # Iterate over vocab/output dimension\n",
        "                for j_h in range(hidden_size): # Iterate over hidden/input dimension\n",
        "                    Wy[i_v][j_h] -= learning_rate * dy[i_v] * h_t[j_h]\n",
        "\n",
        "            # dby = dy\n",
        "            # by shape: [vocab_size]\n",
        "            for i_v in range(vocab_size):\n",
        "                by[i_v] -= learning_rate * dy[i_v]\n",
        "\n",
        "            # Gradient w.r.t. hidden state (h_t)\n",
        "            # dh = Wy^T @ dy\n",
        "            # Wy^T shape: [hidden_size][vocab_size]\n",
        "            # dy shape: [vocab_size]\n",
        "            # dh shape: [hidden_size]\n",
        "            dh = [sum(Wy[i_v][j_h] * dy[i_v] for i_v in range(vocab_size)) for j_h in range(hidden_size)]\n",
        "\n",
        "            # Gradient before activation (tanh)\n",
        "            # dh_raw = dh * dtanh(h_in)\n",
        "            # Using the identity: dtanh(x) = 1 - tanh(x)^2\n",
        "            # Since h_t = tanh(h_in), we can use h_t directly\n",
        "            # dh_raw, dh, h_in, h_t shape: [hidden_size]\n",
        "            dh_raw = [dh[j_h] * (1 - h_t[j_h] ** 2) for j_h in range(hidden_size)]\n",
        "\n",
        "            # Gradients w.r.t. Wx and bh\n",
        "            # dWx = outer(dh_raw, x)\n",
        "            # Wx shape: [hidden_size][vocab_size]\n",
        "            # dh_raw shape: [hidden_size]\n",
        "            # x shape: [vocab_size]\n",
        "            for i_h in range(hidden_size): # Iterate over hidden/output dimension\n",
        "                for j_v in range(vocab_size): # Iterate over vocab/input dimension\n",
        "                    Wx[i_h][j_v] -= learning_rate * dh_raw[i_h] * x[j_v]\n",
        "\n",
        "            # dbh = dh_raw\n",
        "            # bh shape: [hidden_size]\n",
        "            for i_h in range(hidden_size):\n",
        "                bh[i_h] -= learning_rate * dh_raw[i_h]\n",
        "\n",
        "            # Gradient w.r.t. Wh\n",
        "            # dWh = outer(dh_raw, h_prev)\n",
        "            # Wh shape: [hidden_size][hidden_size]\n",
        "            # dh_raw shape: [hidden_size] (for rows)\n",
        "            # h_prev shape: [hidden_size] (for columns)\n",
        "            for i_h in range(hidden_size): # Iterate over current hidden dimension\n",
        "                for j_h_prev in range(hidden_size): # Iterate over previous hidden dimension\n",
        "                    Wh[i_h][j_h_prev] -= learning_rate * dh_raw[i_h] * h_prev[j_h_prev]\n",
        "\n",
        "\n",
        "            h_prev = h_t # Update hidden state for next time step\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ========== 5. After Training ==========\n",
        "print(\"\\n--- Predictions after training ---\\n\")\n",
        "for sentence in data:\n",
        "    words_list = sentence.split()\n",
        "    h_prev = [0] * hidden_size\n",
        "    for i in range(len(words_list) - 1):\n",
        "        current_word = words_list[i]\n",
        "        next_word = words_list[i + 1]\n",
        "\n",
        "        x = one_hot(word_to_idx[current_word], vocab_size)\n",
        "        h_in = vec_add(matvec_mul(Wx, x), vec_add(matvec_mul(Wh, h_prev), bh))\n",
        "        h_t = tanh(h_in)\n",
        "        y_raw = vec_add(matvec_mul(Wy, h_t), by)\n",
        "        y_pred = softmax(y_raw)\n",
        "\n",
        "        predicted_idx = y_pred.index(max(y_pred))\n",
        "        predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "        print(f\"Input: '{current_word}' â†’ Predicted: '{predicted_word}' (Target: '{next_word}')\")\n",
        "\n",
        "        h_prev = h_t\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZDG0212Lo8h",
        "outputId": "82b169a7-d9d8-41fc-b336-74c18823bade"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 6\n",
            "Words: ['are', 'doing', 'hello', 'how', 'well', 'you']\n",
            "Wx dimensions: 8 x 6 (expected 8 x 6)\n",
            "Wh dimensions: 8 x 8 (expected 8 x 8)\n",
            "Wy dimensions: 6 x 8 (expected 6 x 8)\n",
            "bh dimensions: 8 (expected 8)\n",
            "by dimensions: 6 (expected 6)\n",
            "Epoch 0 | Loss: 16.2611\n",
            "Epoch 10 | Loss: 11.3341\n",
            "Epoch 20 | Loss: 3.7659\n",
            "Epoch 30 | Loss: 1.3022\n",
            "Epoch 40 | Loss: 0.6056\n",
            "Epoch 50 | Loss: 0.3696\n",
            "Epoch 60 | Loss: 0.2599\n",
            "Epoch 70 | Loss: 0.1982\n",
            "Epoch 80 | Loss: 0.1591\n",
            "Epoch 90 | Loss: 0.1323\n",
            "\n",
            "--- Predictions after training ---\n",
            "\n",
            "Input: 'hello' â†’ Predicted: 'how' (Target: 'how')\n",
            "Input: 'how' â†’ Predicted: 'are' (Target: 'are')\n",
            "Input: 'are' â†’ Predicted: 'you' (Target: 'you')\n",
            "Input: 'how' â†’ Predicted: 'are' (Target: 'are')\n",
            "Input: 'are' â†’ Predicted: 'you' (Target: 'you')\n",
            "Input: 'you' â†’ Predicted: 'doing' (Target: 'doing')\n",
            "Input: 'are' â†’ Predicted: 'you' (Target: 'you')\n",
            "Input: 'you' â†’ Predicted: 'doing' (Target: 'doing')\n",
            "Input: 'doing' â†’ Predicted: 'well' (Target: 'well')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zX8pGPFZMZ3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}